{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因篇幅原因，对卷积神经网络的讲解详细见我的博客。\n",
    "<br>\n",
    "\n",
    "# <a href=\"https://blog.csdn.net/weixin_43217928/article/details/88426172\" target=\"_blank\">【一文读懂卷积神经网络（一）】可能是你看过的最全的CNN（步长为1，无填充）</a>\n",
    "<br>\n",
    "\n",
    "# <a href=\"https://blog.csdn.net/weixin_43217928/article/details/88555389\" target=\"_blank\">【一文读懂卷积神经网络（二）】可能是你看过的最全的CNN（步长为1，有填充）</a>\n",
    "<br>\n",
    "\n",
    "# <a href=\"https://blog.csdn.net/weixin_43217928/article/details/88564517\" target=\"_blank\">【一文读懂卷积神经网络（三）】可能是你看过的最全的CNN（步长不为1，无填充）</a>\n",
    "<br>\n",
    "\n",
    "# <a href=\"https://blog.csdn.net/weixin_43217928/article/details/88597216\" target=\"_blank\">【一文读懂卷积神经网络（四）】可能是你看过的最全的CNN（步长不为1，有填充）</a>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnnModel():\n",
    "    def __init__(self,conv_layers,fc_layers,Xtrain,ytrain,\n",
    "                 batch_size=16):\n",
    "        self.conv_layers=conv_layers  #卷积层结构\n",
    "        self.fc_layers=fc_layers      #全连接层结构\n",
    "        self.convWeights = [np.random.randn(layer['filterSize'], layer['filterSize'], layer['filterChannels'],layer['filterNums']) for layer in self.conv_layers]\n",
    "        self.convBiases = [np.random.randn(1, layer['filterNums']) for layer in self.conv_layers]\n",
    "        self.fcWeights=[]\n",
    "        self.Xtrain=Xtrain\n",
    "        self.ytrain=ytrain\n",
    "        self.batch_size=batch_size\n",
    "        self.batch_index=0\n",
    "        self.data_index=[i for i in range(Xtrain.shape[0])]\n",
    "        self.caches={}\n",
    "        print(\"initial conv params complete!\")\n",
    "\n",
    "\n",
    "\n",
    "    def initFCParams(self,flattenNums):\n",
    "        #fcWeights非空，说明已经初始化过了\n",
    "        if self.fcWeights:\n",
    "            return\n",
    "\n",
    "        self.fc_layers.insert(0,flattenNums)\n",
    "        self.fcWeights=[np.random.randn(self.fc_layers[dim],self.fc_layers[dim+1])*np.sqrt(2 / self.fc_layers[dim]) for dim in range(len(self.fc_layers)-1)]\n",
    "        self.fcBiases=[np.zeros((1,self.fc_layers[dim])) for dim in range(1,len(self.fc_layers))]\n",
    "        print(\"initial FC params complete!\")\n",
    "\n",
    "\n",
    "\n",
    "    def zero_pad(self,X,pad_size):   #对样本进行填充\n",
    "        #pad_size为tuple\n",
    "        # 填充顺序依次为样本数，高度，宽度，通道数\n",
    "        X_paded=np.pad(X,((0,0),pad_size,pad_size,(0,0)),'constant')\n",
    "        return X_paded\n",
    "\n",
    "\n",
    "    def delete_pad(self,dA,pad_size):\n",
    "        pad_h_size,pad_w_size=pad_size  #填充的大小\n",
    "        m,n_H_prev,n_W_prev,n_C=dA.shape  #填充后的大小\n",
    "        #计算出填充前的大小\n",
    "        n_H=n_H_prev-2*pad_h_size\n",
    "        n_W=n_W_prev-2*pad_w_size\n",
    "        #需要删除的行和列的list\n",
    "        delete_h_list=[h for h in range(pad_h_size)]\n",
    "        delete_w_list=[w for w in range(pad_w_size)]\n",
    "        for i in range(pad_h_size):\n",
    "            delete_h_list.append((i+n_H+pad_h_size))\n",
    "        for j in range(pad_w_size):\n",
    "            delete_w_list.append((j+n_W+pad_w_size))\n",
    "\n",
    "        dA=np.delete(dA,delete_h_list,axis=1)\n",
    "        dA=np.delete(dA,delete_w_list,axis=2)\n",
    "\n",
    "        return dA\n",
    "\n",
    "\n",
    "\n",
    "    def zero_insert(self,dZ,insert_size):\n",
    "        m,H,W,C=dZ.shape\n",
    "        dZout=dZ.copy()\n",
    "        insert_values_h=np.zeros((insert_size,W))\n",
    "        insert_values_w=np.zeros((H,insert_size))\n",
    "        for h in range(H-1):\n",
    "            dZout=np.insert(dZout,h+1,values=insert_values_h,\n",
    "                            axis=1)\n",
    "        for w in range(W-1):\n",
    "            dZout=np.insert(dZout,w+1,values=insert_values_w,\n",
    "                            axis=2)\n",
    "        return dZout\n",
    "\n",
    "\n",
    "\n",
    "    def relu(self,Z):\n",
    "        return np.maximum(0,Z)\n",
    "\n",
    "\n",
    "\n",
    "    def relu_back(self,dA,Z):\n",
    "        dZ=dA.copy()\n",
    "        dZ[Z<=0]=0\n",
    "        return dZ\n",
    "\n",
    "\n",
    "\n",
    "    def softmax(self,Z):\n",
    "        return np.divide(np.exp(Z), np.sum(np.exp(Z), axis=1).reshape(-1,1))\n",
    "\n",
    "\n",
    "    def softmax_back(self,A,y):\n",
    "        return A-y\n",
    "\n",
    "\n",
    "    def max_pool_forward(self,Ain,poolSize,strides=1,\n",
    "                         mode='SAME'):\n",
    "        #Ain输入数据\n",
    "        #dim为当前层数\n",
    "        #poolSize为池化核大小\n",
    "        #strides为池化的步长\n",
    "        #mode为是否填充\n",
    "        batch_size, n_H_prev, n_W_prev, n_C_prev = Ain.shape\n",
    "        # batch_size批量输入数据的个数，n_H_prev输入数据的高，n_W_prev输入数据的宽，n_C_prev输入数据的通道数\n",
    "        # self.caches['unpadAinSize'+str(dim+1)]=Ain.shape\n",
    "        padSize=()\n",
    "        if mode=='SAME':  #如果需要填充的话\n",
    "            pad_h_size = int((strides * (n_H_prev - 1) - n_H_prev + poolSize) / 2)  # 在高度上填充的大小\n",
    "            pad_w_size = int((strides * (n_W_prev - 1) - n_W_prev + poolSize) / 2)  # 在宽度上填充的大小\n",
    "            padSize=(pad_h_size,pad_w_size)\n",
    "            # self.caches['padSize' + str(dim + 1)] = (pad_h_size,pad_w_size)\n",
    "            Ain = self.zero_pad(Ain, pad_size=(pad_h_size, pad_w_size))  # 进行0填充\n",
    "\n",
    "            n_H = n_H_prev  # 卷积后图像的高\n",
    "            n_W = n_W_prev  # 卷积后图像的宽\n",
    "        elif mode=='VALID':\n",
    "            n_H = int(np.floor((n_H_prev - poolSize) / strides) + 1)\n",
    "            # 卷积后图像的高\n",
    "            n_W = int(np.floor((n_W_prev - poolSize) / strides) + 1)\n",
    "            # 卷积后图像的宽\n",
    "        else:\n",
    "            raise Exception(\"Invalid padding method!\")\n",
    "        # self.caches['poolAin_pad' + str(dim + 1)] = Ain\n",
    "        Aout=np.zeros((batch_size,n_H,n_W,n_C_prev))\n",
    "        for i in range(batch_size):\n",
    "            for h in range(n_H):\n",
    "                for w in range(n_W):\n",
    "                    for c in range(n_C_prev):\n",
    "                        h_start=h*strides\n",
    "                        h_end=h_start+poolSize\n",
    "                        w_start=w*strides\n",
    "                        w_end=w_start+poolSize\n",
    "                        Aout[i,h,w,c]=np.max(Ain[i,\n",
    "h_start:h_end,w_start:w_end,c])\n",
    "        #返回的Aout是池化后的结果，如果mode=='SAME',则Ain是填充后的Ain，否则为未填充的Ain，padSize为填充的大小，若未填充则为空\n",
    "        return Aout,Ain,padSize\n",
    "\n",
    "\n",
    "\n",
    "    def average_pool_forward(self,Ain,poolSize,strides=1,\n",
    "                             mode='SAME'):\n",
    "        batch_size, n_H_prev, n_W_prev, n_C_prev = Ain.shape\n",
    "        # batch_size批量输入数据的个数，n_H_prev输入数据的高，n_W_prev输入数据的宽，n_C_prev输入数据的通道数\n",
    "        # self.caches['unpadAinSize' + str(dim + 1)] = Ain.shape\n",
    "        padSize=()\n",
    "        if mode == 'SAME':\n",
    "            pad_h_size = int((strides * (n_H_prev - 1) - n_H_prev + poolSize) / 2)  # 在高度上填充的大小\n",
    "            pad_w_size = int((strides * (n_W_prev - 1) - n_W_prev + poolSize) / 2)  # 在宽度上填充的大小\n",
    "            # self.caches['padSize'+str(dim+1)]=(pad_h_size,pad_w_size)\n",
    "            padSize=(pad_h_size,pad_w_size)\n",
    "            Ain = self.zero_pad(Ain, pad_size=(pad_h_size, pad_w_size))  # 进行0填充\n",
    "            n_H = n_H_prev  # 卷积后图像的高\n",
    "            n_W = n_W_prev  # 卷积后图像的宽\n",
    "        elif mode == 'VALID':\n",
    "            n_H = int(np.floor((n_H_prev - poolSize) / strides) + 1)\n",
    "            # 卷积后图像的高\n",
    "            n_W = int(np.floor((n_W_prev - poolSize) / strides) + 1)\n",
    "            # 卷积后图像的宽\n",
    "        else:\n",
    "            raise Exception(\"Invalid padding method!\")\n",
    "        # self.caches['poolAin_pad' + str(dim + 1)] = Ain\n",
    "        Aout = np.zeros((batch_size, n_H, n_W, n_C_prev))\n",
    "        for i in range(batch_size):\n",
    "            for h in range(n_H):\n",
    "                for w in range(n_W):\n",
    "                    for c in range(n_C_prev):\n",
    "                        h_start = h * strides\n",
    "                        h_end = h_start + poolSize\n",
    "                        w_start = w * strides\n",
    "                        w_end = w_start + poolSize\n",
    "                        Aout[i, h, w, c] = np.mean(Ain[i, h_start:h_end, w_start:w_end, c])\n",
    "        # 返回的Aout是池化后的结果，如果mode=='SAME',则Ain是填充后的Ain，否则为未填充的Ain，padSize为填充的大小，若未填充则为空\n",
    "        return Aout,Ain,padSize\n",
    "\n",
    "\n",
    "\n",
    "    def fc_forward(self,A,predict):    #全连接层前向传播\n",
    "        if not predict:\n",
    "            self.caches['FCA0']=A\n",
    "        for dim in range(1,len(self.fc_layers)):\n",
    "            Z=A.dot(self.fcWeights[dim-1])+self.fcBiases[dim-1]\n",
    "            if dim==(len(self.fc_layers)-1):  #如果是最后一层\n",
    "                A=self.softmax(Z)\n",
    "            else:\n",
    "                A=self.relu(Z)\n",
    "            if not predict:\n",
    "                self.caches['FCA'+str(dim)]=A\n",
    "                self.caches['FCZ'+str(dim)]=Z\n",
    "        return A\n",
    "\n",
    "\n",
    "\n",
    "    def conv_forward(self,A,convFilter,bias,padding='SAME',\n",
    "                     strides=1):   #卷积层前向传播\n",
    "        batch_size, n_H_prev, n_W_prev, n_C_prev = A.shape\n",
    "        # batch_size批量输入数据的个数，n_H_prev输入数据的高，n_W_prev输入数据的宽，n_C_prev输入数据的通道数\n",
    "        filterSize, filterSize, filterChannels, filterNums = convFilter.shape\n",
    "        # filterSize卷积核大小，filterChannels卷积核的通道数，应该和n_C_prev相等，filterNums卷积核个数\n",
    "        padSize=()\n",
    "        if padding=='SAME':\n",
    "            pad_h_size=int((strides*(n_H_prev-1)-n_H_prev+filterSize)/2 ) #在高度上填充的大小\n",
    "            pad_w_size = int((strides * (n_W_prev - 1) - n_W_prev + filterSize) / 2 )  #在宽度上填充的大小\n",
    "            padSize=(pad_h_size,pad_w_size)\n",
    "            A=self.zero_pad(A,pad_size=(pad_h_size,pad_w_size))      #进行0填充\n",
    "            n_H=n_H_prev   #卷积后图像的高\n",
    "            n_W=n_W_prev   #卷积后图像的宽\n",
    "        elif padding=='VALID':\n",
    "            n_H=int(np.floor((n_H_prev-filterSize)/strides)+1)\n",
    "            # 卷积后图像的高\n",
    "            n_W=int(np.floor((n_W_prev-filterSize)/strides)+1)\n",
    "            # 卷积后图像的宽\n",
    "        else:\n",
    "            raise Exception(\"Invalid padding method!\")\n",
    "        #batch_size为一个batch的样本数，n_H为卷积后的高，n_W为卷积后的宽，filterNums为卷积后的通道数\n",
    "        Z=np.zeros((batch_size,n_H,n_W,filterNums))\n",
    "        for c in range(filterNums):\n",
    "            for i in range(batch_size):\n",
    "                for h in range(n_H):\n",
    "                    for w in range(n_W):\n",
    "                        h_start=h*strides\n",
    "                        h_end=h_start+filterSize\n",
    "                        w_start=w*strides\n",
    "                        w_end=w_start+filterSize\n",
    "                        Z[i,h,w,c]=np.sum(\n",
    "np.multiply(A[i,h_start:h_end,w_start:w_end,:],convFilter[:,:,:,c]))+bias[0,c]\n",
    "        #返回卷积后的Z，若padding='SAME'，则A为填充后的A，否则就为传入的A\n",
    "        return Z,A,padSize\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,xs,predict=False):      #模型总的前向传播，由卷积层前向传播与前连接层前向传播组成\n",
    "        Aout=xs\n",
    "\n",
    "        for dim,layer in enumerate(self.conv_layers):\n",
    "            #Aout为上一层池化的输出，如果为第一层则是输入样本\n",
    "            Z,Apad,padSize=self.conv_forward(Aout,\n",
    "self.convWeights[dim],self.convBiases[dim],layer['conv_padding'],layer['strides'])\n",
    "            #Z为卷积后的结果，若layer['conv_padding']=='SAME'，则Apaded为填充后的Aout，若layer['conv_padding']=='VALID'，则Apaded就是Aout\n",
    "\n",
    "            #激活函数\n",
    "            Ain=self.relu(Z)\n",
    "\n",
    "\n",
    "            if layer['poolType']=='MAX':\n",
    "                Aout,Ain_pad,padSize=self.max_pool_forward(Ain,\n",
    "layer['poolSize'],layer['poolSize'],layer['pool_padding'])\n",
    "            else:\n",
    "                Aout,Ain_pad,padSize=self.average_pool_forward(Ain, layer['poolSize'], layer['poolSize'], layer['pool_padding'])\n",
    "            #如果当前不是预测，就将反向传播时需要用的保存起来\n",
    "            if not predict:\n",
    "                self.caches['convPadSize' + str(dim)] = padSize\n",
    "                # 将当前的Apaded储存起来供反向传播时使用\n",
    "                self.caches['convAout' + str(dim)] = Apad\n",
    "                # 将Z保存起来，供反向传播时使用\n",
    "                self.caches['convZ' + str(dim + 1)] = Z\n",
    "                self.caches['poolAin'+str(dim+1)]=Ain_pad\n",
    "                self.caches['poolPadSize'+str(dim+1)]=padSize\n",
    "                self.caches['poolAout'+str(dim+1)]=Aout\n",
    "\n",
    "        #全连接层\n",
    "        A=Aout.reshape((Aout.shape[0],-1))\n",
    "        \n",
    "        if not predict:\n",
    "            self.initFCParams(A.shape[1])\n",
    "\n",
    "        AL=self.fc_forward(A,predict)\n",
    "\n",
    "        return AL\n",
    "\n",
    "\n",
    "    def fc_backward(self,AL,y):   #全连接层的后向传播\n",
    "        grads={}\n",
    "        L = len(self.fc_layers)\n",
    "        m = AL.shape[0]\n",
    "        dZ = self.softmax_back(AL,y)\n",
    "        grads['FCdW' + str(L - 2)] = (1. / m) * self.caches['FCA' + str(L - 2)].T.dot(dZ)\n",
    "        grads['FCdb' + str(L - 2)] = (1. / m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "        dA = dZ.dot(self.fcWeights[L-2].T)\n",
    "        for dim in reversed(range(1, L-1)):\n",
    "            dZ=self.relu_back(dA,self.caches['FCZ'+str(dim)])\n",
    "            grads['FCdW' + str(dim - 1)] = (1. / m) * self.caches['FCA' + str(dim - 1)].T.dot(dZ)\n",
    "            grads['FCdb' + str(dim - 1)] = (1. / m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "            dA = dZ.dot(self.fcWeights[dim-1].T)\n",
    "        return grads,dA\n",
    "\n",
    "\n",
    "\n",
    "    def conv_backward(self,dZ,convFilter,Aout,filterSize,\n",
    "                      strides):   #卷积层的后向传播\n",
    "        #对卷积核进行顺时针翻转180度\n",
    "        f_i,f_j,f_k,f_l=convFilter.shape\n",
    "        for k in range(f_k):\n",
    "            for l in range(f_l):\n",
    "                convFilter[:,:,k,l]=np.rot90(convFilter[:,:,k,l],\n",
    "k=-1)\n",
    "                convFilter[:,:,k,l]=np.rot90(convFilter[:,:,k,l],\n",
    "k=-1)\n",
    "\n",
    "\n",
    "        batch_size,n_H_prev,n_W_prev,n_C_prev=dZ.shape\n",
    "        batch_size,n_H,n_W,n_C=Aout.shape\n",
    "        dW = np.zeros((filterSize, filterSize, n_C, n_C_prev))\n",
    "        for num in range(n_C_prev):  \n",
    "            for c in range(n_C):\n",
    "                for m in range(filterSize):\n",
    "                    for n in range(filterSize):\n",
    "                        for i in range(n_H_prev):\n",
    "                            for j in range(n_W_prev):\n",
    "                                dW[m, n, c, num] += np.sum(np.multiply(Aout[:, i * strides + m, j * strides + n, c], dZ[:, i, j, num]))\n",
    "                                \n",
    "        dW = dW / batch_size\n",
    "        db = np.sum(np.sum(np.sum(dZ, axis=0), axis=0), axis=0, keepdims=True) / batch_size\n",
    "\n",
    "        dA=np.zeros_like(Aout)\n",
    "        if strides>1:\n",
    "            dZ_inserted = self.zero_insert(dZ, strides - 1)\n",
    "            batch_size, n_H_prev, n_W_prev, n_C_prev=dZ_inserted.shape\n",
    "        else:\n",
    "            dZ_inserted=dZ\n",
    "        pad_h_size = int((n_H - 1 - n_H_prev + filterSize) / 2)\n",
    "        pad_w_size = int((n_W - 1 - n_W_prev + filterSize) / 2)\n",
    "        dZ_inserted_paded=self.zero_pad(dZ_inserted,pad_size=(pad_h_size,pad_w_size))\n",
    "        for c in range(n_C):  \n",
    "            for i in range(batch_size):  \n",
    "                for h in range(n_H):  \n",
    "                    for w in range(n_W):  \n",
    "                        dA[i,h,w,c]=np.sum(\n",
    "np.multiply(dZ_inserted_paded[i,h:h+filterSize,w:w+filterSize,:],convFilter[:,:,c,:]))\n",
    "\n",
    "        return dA,dW,db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def max_pool_backward(self,dAout,Ain,filterSize,strides):\n",
    "        #正向传播时的Ain，如果正向传播时做了填充，这就是填充后的Ain，如果没有，就是未填充的Ain\n",
    "        dAin=np.zeros_like(Ain)\n",
    "        m,n_H,n_W,n_C=dAout.shape\n",
    "        for i in range(m):\n",
    "            for h in range(n_H):\n",
    "                for w in range(n_W):\n",
    "                    for c in range(n_C):\n",
    "                        h_start=h*strides\n",
    "                        h_end=h_start+filterSize\n",
    "                        w_start=w*strides\n",
    "                        w_end=w_start+filterSize\n",
    "                        mask=(Ain[i,h_start:h_end,\n",
    "w_start:w_end,c]==np.max(Ain[i,h_start:h_end,w_start:w_end,c]))\n",
    "                        dAin[i,h_start:h_end,\n",
    "w_start:w_end,c]=mask*dAout[i,h,w,c]\n",
    "        return dAin\n",
    "\n",
    "\n",
    "\n",
    "    def average_pool_backward(self,dAout,Ain,filterSize,strides):\n",
    "        # 正向传播时的Ain，如果正向传播时做了填充，这就是填充后的Ain，如果没有，就是未填充的Ain\n",
    "        dAin = np.zeros_like(Ain)\n",
    "        m, n_H, n_W, n_C = dAout.shape\n",
    "        for i in range(m):\n",
    "            for h in range(n_H):\n",
    "                for w in range(n_W):\n",
    "                    for c in range(n_C):\n",
    "                        h_start = h * strides\n",
    "                        h_end = h_start + filterSize\n",
    "                        w_start = w * strides\n",
    "                        w_end = w_start + filterSize\n",
    "                        dAin[i,h_start:h_end,\n",
    "w_start:w_end,c]+=np.ones((filterSize,filterSize))*(dAout[i,h,w,c]/(filterSize**2))\n",
    "\n",
    "        return dAin\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self,AL,y):      #模型总得后向传播，由全连接层的后向传播与卷积层的后向传播组成\n",
    "        grads,dA=self.fc_backward(AL,y)\n",
    "        L=len(self.conv_layers)\n",
    "        dAout=dA.reshape(self.caches['poolAout'+str(L)].shape)\n",
    "        for dim in reversed(range(L)):\n",
    "            layer=self.conv_layers[dim]\n",
    "            \n",
    "            if layer['poolType']=='MAX':\n",
    "                dAin=self.max_pool_backward(dAout,\n",
    "self.caches['poolAin'+str(dim+1)],layer['poolSize'],layer['poolSize'])\n",
    "            else:\n",
    "                dAin = self.average_pool_backward(dAout, self.caches['poolAin' + str(dim + 1)], layer['poolSize'],\n",
    "layer['poolSize'])\n",
    "                \n",
    "            if layer['pool_padding']=='SAME':\n",
    "                dAin=self.delete_pad(dAin,\n",
    "pad_size=self.caches['poolPadSize' + str(dim + 1)])\n",
    "                \n",
    "            dZ=self.relu_back(dAin,self.caches['convZ'+str(dim+1)])\n",
    "            dAout,dW,db=self.conv_backward(dZ,\n",
    "self.convWeights[dim],self.caches['convAout'+str(dim)],layer['filterSize'],layer['strides'])\n",
    "            \n",
    "            if layer['conv_padding']=='SAME':\n",
    "                dAout=self.delete_pad(dAout,\n",
    "self.caches['convPadSize'+str(dim)])\n",
    "\n",
    "            grads['CONVdW'+str(dim)]=dW\n",
    "            grads['CONVdb'+str(dim)]=db\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def updateParams(self,grads):   #更新参数，在后向传播结束时进行\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            self.convWeights[i]=self.convWeights[i]-self.learning_rate*grads['CONVdW'+str(i)]\n",
    "            self.convBiases[i]=self.convBiases[i]-self.learning_rate*grads['CONVdb'+str(i)]\n",
    "\n",
    "        for j in range(len(self.fc_layers)-1):\n",
    "            self.fcWeights[j]=self.fcWeights[j]-self.learning_rate*grads['FCdW'+str(j)]\n",
    "            self.fcBiases[j]=self.fcBiases[j]-self.learning_rate*grads['FCdb'+str(j)]\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        m=self.Xtrain.shape[0]\n",
    "        start = min(self.batch_index, m)\n",
    "        if start==m:\n",
    "            self.batch_index=0\n",
    "            start=self.batch_index\n",
    "        if self.batch_index==0:  #一轮数据遍历完了，重新打乱数据\n",
    "            random.shuffle(self.data_index)\n",
    "        end=min(start+self.batch_size,m)\n",
    "        xs=self.Xtrain[start:end]\n",
    "        ys=self.ytrain[start:end]\n",
    "        self.batch_index=end\n",
    "        return xs,ys\n",
    "\n",
    "\n",
    "    def GredientDescentOptimizer(self,learning_rate,\n",
    "                                 training_steps=200):\n",
    "        self.learning_rate=learning_rate\n",
    "        xs,ys=self.next_batch()\n",
    "        for step in range(training_steps):\n",
    "            AL=self.forward(xs)\n",
    "            grads=self.backward(AL,ys)\n",
    "            self.updateParams(grads)\n",
    "            if step%5==0:\n",
    "                accuracy=self.calcAccuracy(\n",
    "self.Xtrain[:100,:,:,:],self.ytrain[:100,:])\n",
    "                loss=self.calcLoss(\n",
    "self.Xtrain[:100,:,:,:],self.ytrain[:100,:])\n",
    "                print(\"after %d training steps,the accuracy on test dataset is :%g%%,the loss is :%g\"%(step,accuracy,loss))\n",
    "        print(\"training complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calcAccuracy(self,X,y):   #给定输入数据和标签，计算当前模型下的正确率\n",
    "        h=np.argmax(self.forward(X,predict=True),1).reshape(-1,1)\n",
    "        y=np.argmax(y,1).reshape(-1,1)\n",
    "        accuracy=np.sum(h==y)/len(h)*100\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def calcLoss(self,X,y):    #给定输入数据和标签，计算当前模型下的损失\n",
    "\n",
    "        h=self.forward(X,predict=True)\n",
    "        h=np.log(h)\n",
    "        loss=np.sum(np.multiply(h,-y))/y.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据的function\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据\n",
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=load_dataset()\n",
    "\n",
    "trainX=train_set_x_orig/255  #shape(1080,64,64,3)样本数，高，宽，通道数\n",
    "testX=test_set_x_orig/255\n",
    "trainy=train_set_y_orig.T\n",
    "testy=test_set_y_orig.T\n",
    "#转为one-hot编码\n",
    "n_class = testy.max() + 1\n",
    "trainy=np.eye(n_class)[trainy].reshape(-1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 64, 64, 3)\n",
      "(1080, 6)\n",
      "(120, 64, 64, 3)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainy.shape)\n",
    "print(testX.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造网络层结构的function\n",
    "def generateNNLayerStruct():\n",
    "    conv_layers=[]\n",
    "    cnnLayerNums=int(input(\"请输入卷积层的层数(please input your convolution layer's nums)：\"))\n",
    "    print(\"您为卷积层设置了%d层(You set %d layers for convolution)\"%(cnnLayerNums,cnnLayerNums))\n",
    "    for layer in range(int(cnnLayerNums)):\n",
    "        dict={}\n",
    "        print(\"\\n\")\n",
    "        print(\"请输入第%d层的参数(please input %d layer's params):\"%(layer+1,layer+1))\n",
    "        print(\"\\n\")\n",
    "        #卷积层参数\n",
    "        dict['filterSize']=int(input(\"请输入当前层卷积核的大小(please input current layer's convolution filter size):\"))\n",
    "        dict['filterChannels']=int(input(\"请输入当前层卷积核的通道数，必须和输入数据的通道数相同(please input current layer's convolution filter channels,it must be equal to input data's channels):\"))\n",
    "        dict['filterNums']=int(input(\"请输入当前层卷积核的数目(please input current layer's convolution filter nums):\"))\n",
    "        dict['conv_padding']=str(input(\"请输入当前层卷积层的填充模式，分为SAME（填充）和VALID（不填充）两种(please input current layer's convolution layer's pad mode-SAME(pad) or VALID(non-pad)):\"))\n",
    "        dict['strides']=int(input(\"请输入当前层卷积层的步长(please input current layer's convolution layer's strides):\"))\n",
    "        #池化层参数\n",
    "        dict['poolSize']=int(input(\"请输入当前层池化核的大小(please input current layer's pooling filter size):\"))\n",
    "        dict['poolType']=str(input(\"请输入当前层池化的模式，MAX（最大池化）和AVERAGE（均值池化）两种(please input current layer's pooling layer's mode-MAX(max pooling) or AVERAGE(average pooling)):\"))\n",
    "        dict['pool_padding']=str(input(\"请输入当前层池化的填充模式，分为SAME（填充）和VALID（不填充）两种(please input current layer's pooling layer's pad mode-SAME(pad) or VALID(non-pad)):\"))\n",
    "        conv_layers.append(dict)\n",
    "    \n",
    "    print(\"\\n卷积层参数输入完毕!（input convolution layer's params complete!）\\n\")\n",
    "    \n",
    "    fc_layers=[]\n",
    "    fcLayerNums=int(input(\"请输入全连接层的层数，只用计算输入隐藏层和输出层，不用输入层(please input your full connect layer's nums,only count hidden layers and output layers)：\"))\n",
    "    print(\"您为全连接层设置了%d层(You set %d layers for full connect)\"%(fcLayerNums,fcLayerNums))\n",
    "    for layer in range(int(fcLayerNums)):\n",
    "        print(\"\\n\")\n",
    "        print(\"请输入第%d层的节点数(please input %d layer's node nums):\"%(layer+1,layer+1))\n",
    "        print(\"\\n\")\n",
    "        nums=input(\"请输入当前层的节点数(please input current layer's node nums)：\")\n",
    "        fc_layers.append(int(nums))\n",
    "    \n",
    "    print(\"\\n全连接层参数输入完毕!（input full connect layer's params complete!）\\n\")\n",
    "    \n",
    "    return conv_layers,fc_layers\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入卷积层的层数(please input your convolution layer's nums)：2\n",
      "您为卷积层设置了2层(You set 2 layers for convolution)\n",
      "\n",
      "\n",
      "请输入第1层的参数(please input 1 layer's params):\n",
      "\n",
      "\n",
      "请输入当前层卷积核的大小(please input current layer's convolution filter size):5\n",
      "请输入当前层卷积核的通道数，必须和输入数据的通道数相同(please input current layer's convolution filter channels,it must be equal to input data's channels):3\n",
      "请输入当前层卷积核的数目(please input current layer's convolution filter nums):8\n",
      "请输入当前层卷积层的填充模式，分为SAME（填充）和VALID（不填充）两种(please input current layer's convolution layer's pad mode-SAME(pad) or VALID(non-pad)):SAME\n",
      "请输入当前层卷积层的步长(please input current layer's convolution layer's strides):1\n",
      "请输入当前层池化核的大小(please input current layer's pooling filter size):8\n",
      "请输入当前层池化的模式，MAX（最大池化）和AVERAGE（均值池化）两种(please input current layer's pooling layer's mode-MAX(max pooling) or AVERAGE(average pooling)):MAX\n",
      "请输入当前层池化的填充模式，分为SAME（填充）和VALID（不填充）两种(please input current layer's pooling layer's pad mode-SAME(pad) or VALID(non-pad)):SAME\n",
      "\n",
      "\n",
      "请输入第2层的参数(please input 2 layer's params):\n",
      "\n",
      "\n",
      "请输入当前层卷积核的大小(please input current layer's convolution filter size):3\n",
      "请输入当前层卷积核的通道数，必须和输入数据的通道数相同(please input current layer's convolution filter channels,it must be equal to input data's channels):8\n",
      "请输入当前层卷积核的数目(please input current layer's convolution filter nums):16\n",
      "请输入当前层卷积层的填充模式，分为SAME（填充）和VALID（不填充）两种(please input current layer's convolution layer's pad mode-SAME(pad) or VALID(non-pad)):SAME\n",
      "请输入当前层卷积层的步长(please input current layer's convolution layer's strides):1\n",
      "请输入当前层池化核的大小(please input current layer's pooling filter size):4\n",
      "请输入当前层池化的模式，MAX（最大池化）和AVERAGE（均值池化）两种(please input current layer's pooling layer's mode-MAX(max pooling) or AVERAGE(average pooling)):MAX\n",
      "请输入当前层池化的填充模式，分为SAME（填充）和VALID（不填充）两种(please input current layer's pooling layer's pad mode-SAME(pad) or VALID(non-pad)):SAME\n",
      "\n",
      "卷积层参数输入完毕!（input convolution layer's params complete!）\n",
      "\n",
      "请输入全连接层的层数，只用计算输入隐藏层和输出层，不用输入层(please input your full connect layer's nums,only count hidden layers and output layers)：1\n",
      "您为全连接层设置了1层(You set 1 layers for full connect)\n",
      "\n",
      "\n",
      "请输入第1层的节点数(please input 1 layer's node nums):\n",
      "\n",
      "\n",
      "请输入当前层的节点数(please input current layer's node nums)：6\n",
      "\n",
      "全连接层参数输入完毕!（input full connect layer's params complete!）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# conv_layers=[{'filterSize':5,'filterChannels':3,'filterNums':8,'conv_padding':'SAME','strides':1,'poolSize':8,'poolType':'MAX','pool_padding':'SAME'},{'filterSize':3,'filterChannels':8,'filterNums':16,'conv_padding':'SAME','strides':1,'poolSize':4,'poolType':'MAX','pool_padding':'SAME'}]\n",
    "# fc_layers=[6]\n",
    "\n",
    "#调用generateNNLayerStruct来设定网络层参数\n",
    "conv_layers,fc_layers=generateNNLayerStruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCNN=cnnModel(conv_layers,fc_layers,trainX,trainy)\n",
    "myCNN.GredientDescentOptimizer(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因为自己实现的代码有大量的for循环,且没有gpu加速，就不在这跑了，本人做了个实验，大小为100的一个batch大约要跑接近1个小时左右，完完全全被tensorflow-gpu完爆了，该代码只是用于学习加深了解卷积神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "batch_size=100\n",
    "IMAGE_SIZE=64\n",
    "NUM_CHANNELS=3\n",
    "OUTPUT_NODE=6\n",
    "BATCH_INDEX=0\n",
    "\n",
    "\n",
    "\n",
    "def getBatch(indexs,batch_size):\n",
    "    global BATCH_INDEX\n",
    "    if BATCH_INDEX==len(indexs):\n",
    "        BATCH_INDEX=0\n",
    "        random.shuffle(indexs)\n",
    "    end=min(BATCH_INDEX+batch_size,len(indexs))\n",
    "    batchIndexs=indexs[BATCH_INDEX:end]\n",
    "    BATCH_INDEX=end\n",
    "    \n",
    "    return batchIndexs,indexs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def forward(conv_layers,fc_layers,X,firstFC):\n",
    "    count = 1\n",
    "    pool=X\n",
    "    for dim,layer in enumerate(conv_layers):\n",
    "        # 卷积层\n",
    "        with tf.variable_scope(('layer'+str(count)+'-conv'+str(dim+1)),reuse=tf.AUTO_REUSE):\n",
    "            conv_weights=tf.get_variable(\"weights\",[layer['filterSize'],layer['filterSize'],layer['filterChannels'],layer['filterNums']],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            conv_biases=tf.get_variable(\"biases\",layer['filterNums'],initializer=tf.constant_initializer(0.0))\n",
    "            conv=tf.nn.conv2d(pool,conv_weights,[1,layer['strides'],layer['strides'],1],padding=layer['conv_padding'])\n",
    "            relu=tf.nn.relu(tf.nn.bias_add(conv,conv_biases))\n",
    "            count+=1\n",
    "\n",
    "        #池化层\n",
    "        with tf.variable_scope(('layer'+str(count)+'-pool'+str(dim+1)),reuse=tf.AUTO_REUSE):\n",
    "            if layer['poolType']=='MAX':\n",
    "                pool=tf.nn.max_pool(relu,[1,layer['poolSize'],layer['poolSize'],1],[1,layer['poolSize'],layer['poolSize'],1],layer['pool_padding'])\n",
    "            else:\n",
    "                pool = tf.nn.avg_pool(relu, [1, layer['poolSize'], layer['poolSize'], 1], [1,layer['poolSize'],layer['poolSize'],1],layer['pool_padding'])\n",
    "            count+=1\n",
    "\n",
    "\n",
    "    pool_shape=pool.get_shape().as_list()\n",
    "    #展开\n",
    "    nodes=pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "    flatten=tf.reshape(pool,[pool_shape[0],nodes])\n",
    "    FCLAYERS=fc_layers.copy()\n",
    "    if firstFC:\n",
    "        FCLAYERS.insert(0,nodes)\n",
    "        firstFC==False\n",
    "    for dim in range(len(FCLAYERS)-1):\n",
    "        with tf.variable_scope(('layer'+str(count)+'-fc'+str(dim+1)),reuse=tf.AUTO_REUSE):\n",
    "            fc_weights=tf.get_variable(\"weights\",[FCLAYERS[dim],FCLAYERS[dim+1]],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            fc_biases=tf.get_variable(\"biases\",\n",
    "FCLAYERS[dim+1],initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "            flatten=tf.matmul(flatten,fc_weights)+fc_biases\n",
    "            if dim!=(len(FCLAYERS)-2):\n",
    "                flatten=tf.nn.relu(flatten)\n",
    "            count+=1\n",
    "\n",
    "    logit=flatten\n",
    "    return logit\n",
    "\n",
    "\n",
    "\n",
    "def train(conv_layers,fc_layers,Xtrain,ytrain,learning_rate,\n",
    "          TRAINING_STEPS=1000):\n",
    "    X=tf.placeholder(tf.float32,[batch_size,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS],name=\"Xinput\")\n",
    "    y=tf.placeholder(tf.float32,[None,OUTPUT_NODE],name=\"yinput\")\n",
    "    h=forward(conv_layers,fc_layers,X,True)\n",
    "    cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h,labels=tf.argmax(y,1))\n",
    "    loss=tf.reduce_mean(cross_entropy)\n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        indexs=[i for i in range(X.shape[0])]\n",
    "        for step in range(TRAINING_STEPS):\n",
    "            batchIndexs,indexs=getBatch(indexs,batch_size)\n",
    "            _,loss_value=sess.run([train_step,loss],feed_dict={X:Xtrain[batchIndexs,:,:,:],y:ytrain[batchIndexs,:]})\n",
    "            if step%100==0:\n",
    "                print(\"after %d training steps, loss on training batch is %g\" % (step, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 training steps, loss on training batch is 1.79522\n",
      "after 100 training steps, loss on training batch is 1.73289\n",
      "after 200 training steps, loss on training batch is 1.43567\n",
      "after 300 training steps, loss on training batch is 0.871796\n",
      "after 400 training steps, loss on training batch is 0.365442\n",
      "after 500 training steps, loss on training batch is 0.0639015\n",
      "after 600 training steps, loss on training batch is 0.0224951\n",
      "after 700 training steps, loss on training batch is 0.0126631\n",
      "after 800 training steps, loss on training batch is 0.00854564\n",
      "after 900 training steps, loss on training batch is 0.00633865\n"
     ]
    }
   ],
   "source": [
    "train(conv_layers,fc_layers,trainX,trainy,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
