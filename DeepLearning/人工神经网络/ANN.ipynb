{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  本网络数据集采用MNIST手写数字识别数据集(0-9)，输入为Xtrain(55000,784),标签为ytrain(55000,10)，其中每一个样本的标签为一个行向量，采用one-hot编码，如[0,1,0,0,0,0,0,0,0,0]代表为数字1。网络结构总共为3层，第一层为输入层，采用784个结点，第二层为隐藏层，采用500个结点，第三层为输出层，采用10个结点。\n",
    "\n",
    "\n",
    "因数据量太大，采用随机梯度下降法（SGD）来更新参数，每次所需的batch从next_batch()方法获得（mnist数据集自带next_batch()方法，但本模型并没有使用）\n",
    "\n",
    "\n",
    "训练优化方法有四个：\n",
    "\n",
    "\n",
    "1.GradientDescentOptimizer\n",
    "\n",
    "\n",
    "2.MomentumOptimizer\n",
    "\n",
    "\n",
    "3.RMSpropOptimizer\n",
    "\n",
    "\n",
    "4.AdamOptimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练一个模型的方法为初始化一个annModel(**args)，参数分别为网络层结构，训练集样本，训练集标签，及batch大小（选传，默认为100）。\n",
    "\n",
    "然后选用一个优化方法即可开始训练。\n",
    "\n",
    "GradientDescentOptimizer需要传入的参数为学习率α，训练次数（选传，默认为10000）\n",
    "\n",
    "MomentumOptimizer需要传入的参数为学习率α，β1（选传，默认为0.9），训练次数（选传，默认为10000）\n",
    "\n",
    "RMSpropOptimizer需要传入的参数为学习率α，β2（选传，默认为0.999），epsilon（选传，默认为1e-8），训练次数（选传，默认为10000）\n",
    "\n",
    "AdamOptimizer需要传入的参数为学习率α，β1（选传，默认为0.9），β2（选传，默认为0.999），epsilon（选传，默认为1e-8），训练次数（选传，默认为10000）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "layerDims=[784,500,10]  #网络层结构\n",
    "Xtrain=mnist.train.images  #训练集数据\n",
    "ytrain=mnist.train.labels  #训练集标签\n",
    "\n",
    "Xvalidate=mnist.validation.images  #验证集数据\n",
    "yvalidate=mnist.validation.labels  #验证集标签\n",
    "\n",
    "Xtest=mnist.test.images  #测试集数据\n",
    "ytest=mnist.test.labels  #测试集标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class annModel():\n",
    "    def __init__(self,layerDims,Xtrain,ytrain,batch_size=100):\n",
    "        self.Xtrain=Xtrain\n",
    "        self.ytrain=ytrain\n",
    "        self.data_index = [i for i in range(self.Xtrain.shape[0])]\n",
    "        self.layerDims=layerDims\n",
    "        self.batch_index=0\n",
    "        self.batch_size=batch_size\n",
    "        self.weights=[np.random.randn(layerDims[dim+1],layerDims[dim])*np.sqrt(2 / layerDims[dim]) for dim in range(len(layerDims)-1)]\n",
    "        self.biases=[np.zeros((layerDims[dim],1)) for dim in range(1,len(layerDims))]\n",
    "        self.caches={}\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,X,predict=False):\n",
    "        L=len(self.layerDims)\n",
    "        A=X\n",
    "        if not predict:\n",
    "            self.caches['A0']=A\n",
    "        active_func=self.relu\n",
    "        for dim in range(1,L):\n",
    "            Z=self.weights[dim-1].dot(A)+self.biases[dim-1]\n",
    "            if dim==(L-1):\n",
    "                active_func=self.softmax\n",
    "            A=active_func(Z)\n",
    "            if not predict:\n",
    "                self.caches['Z'+str(dim)]=Z\n",
    "                self.caches['A'+str(dim)]=A\n",
    "        return A\n",
    "\n",
    "\n",
    "\n",
    "    def relu(self,Z):\n",
    "        return np.maximum(0,Z)\n",
    "\n",
    "\n",
    "    def relu_back(self,dA,Z):\n",
    "        dZ=dA.copy()\n",
    "        dZ[Z<=0]=0\n",
    "        return dZ\n",
    "\n",
    "\n",
    "    def softmax(self,Z):\n",
    "        return np.divide(np.exp(Z),np.sum(np.exp(Z),axis=0).reshape(1,-1))\n",
    "\n",
    "\n",
    "    def softmax_back(self,A,y):\n",
    "        return A-y\n",
    "\n",
    "\n",
    "    def backward(self,A,y,method='GradientDescent',beta1=0.9,beta2=0.999,epsilon=1e-8,t=0):\n",
    "        m=A.shape[1]\n",
    "        L=len(self.layerDims)\n",
    "        for dim in reversed(range(L-1)):\n",
    "            if dim == (L-2):\n",
    "                active_func = self.softmax_back\n",
    "                dZ=active_func(A,y)\n",
    "            else:\n",
    "                active_func=self.relu_back\n",
    "                dZ = active_func(dA, self.caches['Z'+str(dim+1)])\n",
    "            dW=(1./m)*dZ.dot(self.caches['A'+str(dim)].T)\n",
    "            db=(1./m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "            dA=self.weights[dim].T.dot(dZ)\n",
    "            self.updateParams(dW,db,dim,method,beta1,beta2,epsilon,t)\n",
    "\n",
    "\n",
    "\n",
    "    def updateParams(self,dW,db,dim,method,beta1,beta2,epsilon,t):\n",
    "        if method=='GradientDescent':\n",
    "            self.weights[dim]=self.weights[dim]-self.learning_rate*dW\n",
    "            self.biases[dim]=self.biases[dim]-self.learning_rate*db\n",
    "            \n",
    "        elif method=='Momentum':\n",
    "            Vdw=beta1*self.caches['Vdw' + str(dim)]+(1-beta1)*dW\n",
    "            Vdb=beta1*self.caches['Vdb' + str(dim)]+(1-beta1)*db\n",
    "            self.weights[dim] = self.weights[dim] - self.learning_rate * Vdw\n",
    "            self.biases[dim] = self.biases[dim] - self.learning_rate * Vdb\n",
    "            self.caches['Vdw' + str(dim)] = Vdw\n",
    "            self.caches['Vdb' + str(dim)] = Vdb\n",
    "            \n",
    "        elif method=='RMSprop':\n",
    "            Sdw = beta2 * self.caches['Sdw' + str(dim)] + (1 - beta2) * np.square(dW)\n",
    "            Sdb = beta2 * self.caches['Sdb' + str(dim)] + (1 - beta2) * np.square(db)\n",
    "            self.weights[dim] = self.weights[dim] - self.learning_rate * (dW/(np.sqrt(Sdw+epsilon)))\n",
    "            self.biases[dim] = self.biases[dim] - self.learning_rate * (db/(np.sqrt(Sdb+epsilon)))\n",
    "            self.caches['Sdw' + str(dim)] = Sdw\n",
    "            self.caches['Sdb' + str(dim)] = Sdb\n",
    "            \n",
    "        elif method=='Adam':\n",
    "            Vdw = beta1 * self.caches['Vdw'+str(dim)] + (1 - beta1) * dW\n",
    "            Vdb = beta1 * self.caches['Vdb'+str(dim)] + (1 - beta1) * db\n",
    "            Sdw = beta2 * self.caches['Sdw'+str(dim)] + (1 - beta2) * np.square(dW)\n",
    "            Sdb = beta2 * self.caches['Sdb'+str(dim)] + (1 - beta2) * np.square(db)\n",
    "            Vdw_correctd=Vdw/(1-pow(beta1,t))\n",
    "            Vdb_corrected=Vdb/(1-pow(beta1,t))\n",
    "            Sdw_corrected=Sdw/(1-pow(beta2,t))\n",
    "            Sdb_corrected=Sdb/(1-pow(beta2,t))\n",
    "            self.weights[dim] = self.weights[dim] - self.learning_rate * (Vdw_correctd / (np.sqrt(Sdw_corrected)+epsilon))\n",
    "            self.biases[dim] = self.biases[dim] - self.learning_rate * (Vdb_corrected / (np.sqrt(Sdb_corrected) + epsilon))\n",
    "            self.caches['Vdw'+str(dim)] = Vdw\n",
    "            self.caches['Vdb'+str(dim)] = Vdb\n",
    "            self.caches['Sdw'+str(dim)] = Sdw\n",
    "            self.caches['Sdb'+str(dim)] = Sdb\n",
    "\n",
    "\n",
    "\n",
    "    def GradientDescentOptimizer(self,learning_rate,training_steps=10000):\n",
    "        self.learning_rate=learning_rate\n",
    "        for step in range(training_steps):\n",
    "            xs, ys = self.next_batch()\n",
    "            xs=xs.T\n",
    "            ys=ys.T\n",
    "            AL=self.forward(xs)\n",
    "            self.backward(AL,ys)\n",
    "            if step %1000==0:\n",
    "                accuracy=self.calcAcc(Xvalidate,yvalidate)*100\n",
    "                loss=self.calcLoss(Xvalidate,yvalidate)\n",
    "                print(\"after %d training steps, training accuracy on validate dataset is %f%%,loss is %f\" % (step, accuracy,loss))\n",
    "        test_accuracy=self.calcAcc(Xtest,ytest)*100\n",
    "        print(\"training complete!Test accuracy is: \",test_accuracy)\n",
    "\n",
    "\n",
    "        \n",
    "    def MomentumOptimizer(self,learning_rate,beta1=0.9,training_steps=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        for l in range(len(self.layerDims)-1):\n",
    "            self.caches['Vdw'+str(l)]=np.zeros_like(self.weights[l])\n",
    "            self.caches['Vdb'+str(l)]=np.zeros_like(self.biases[l])\n",
    "        for step in range(training_steps):\n",
    "            xs, ys = self.next_batch()\n",
    "            xs=xs.T\n",
    "            ys=ys.T\n",
    "            AL = self.forward(xs)\n",
    "            self.backward(AL, ys,'Momentum',beta1=beta1)\n",
    "            if step % 1000 == 0:\n",
    "                accuracy = self.calcAcc(Xvalidate, yvalidate)*100\n",
    "                loss = self.calcLoss(Xvalidate, yvalidate)\n",
    "                print(\"after %d training steps, training accuracy on validate dataset is %g%%,loss is %f\" % (step, accuracy, loss))\n",
    "        test_accuracy=self.calcAcc(Xtest,ytest)*100\n",
    "        print(\"training complete!Test accuracy is: \",test_accuracy)\n",
    "\n",
    "\n",
    "        \n",
    "    def RMSpropOptimizer(self,learning_rate,beta2=0.999,epsilon=1e-8,training_steps=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        for l in range(len(self.layerDims)-1):\n",
    "            self.caches['Sdw'+str(l)]=np.zeros_like(self.weights[l])\n",
    "            self.caches['Sdb'+str(l)]=np.zeros_like(self.biases[l])\n",
    "        for step in range(training_steps):\n",
    "            xs, ys = self.next_batch()\n",
    "            xs=xs.T\n",
    "            ys=ys.T\n",
    "            AL = self.forward(xs)\n",
    "            self.backward(AL, ys,'RMSprop',beta2=beta2,epsilon=epsilon)\n",
    "            if step % 1000 == 0:\n",
    "                accuracy = self.calcAcc(Xvalidate, yvalidate)*100\n",
    "                loss = self.calcLoss(Xvalidate, yvalidate)\n",
    "                print(\"after %d training steps, training accuracy on validate dataset is %g%%,loss is %f\" % (step, accuracy, loss))\n",
    "        test_accuracy=self.calcAcc(Xtest,ytest)*100\n",
    "        print(\"training complete!Test accuracy is: \",test_accuracy)\n",
    "\n",
    "        \n",
    "        \n",
    "    def AdamOptimizer(self,learning_rate,beat1=0.9,beta2=0.999,epsilon=1e-8,training_steps=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        for l in range(len(self.layerDims)-1):\n",
    "            self.caches['Vdw'+str(l)]=np.zeros_like(self.weights[l])\n",
    "            self.caches['Vdb'+str(l)]=np.zeros_like(self.biases[l])\n",
    "            self.caches['Sdw'+str(l)]=np.zeros_like(self.weights[l])\n",
    "            self.caches['Sdb'+str(l)]=np.zeros_like(self.biases[l])\n",
    "        t=0\n",
    "        for step in range(training_steps):\n",
    "            t=t+1\n",
    "            xs, ys = self.next_batch()\n",
    "            xs=xs.T\n",
    "            ys=ys.T\n",
    "            AL = self.forward(xs)\n",
    "            self.backward(AL, ys,'Adam',beta1=beat1,beta2=beta2,epsilon=epsilon,t=t)\n",
    "            if step % 1000 == 0:\n",
    "                accuracy = self.calcAcc(Xvalidate, yvalidate)*100\n",
    "                loss = self.calcLoss(Xvalidate, yvalidate)\n",
    "                print(\"after %d training steps, training accuracy on validate dataset is %g%%,loss is %f\" % (step, accuracy, loss))\n",
    "        test_accuracy=self.calcAcc(Xtest,ytest)*100\n",
    "        print(\"training complete!Test accuracy is: \",test_accuracy)\n",
    "\n",
    "\n",
    "        \n",
    "    def next_batch(self):\n",
    "        m=self.Xtrain.shape[0]\n",
    "        start = min(self.batch_index, m)\n",
    "        if start==m:\n",
    "            self.batch_index=0\n",
    "            start=self.batch_index\n",
    "        if self.batch_index==0:  #一轮数据遍历完了，重新打乱数据\n",
    "            random.shuffle(self.data_index)\n",
    "        end=min(start+self.batch_size,m)\n",
    "        xs=self.Xtrain[start:end]\n",
    "        ys=self.ytrain[start:end]\n",
    "        self.batch_index=end\n",
    "        return xs,ys\n",
    "\n",
    "    \n",
    "\n",
    "    def calcAcc(self,X,Y):\n",
    "        Y_=np.argmax(self.forward(X.T,predict=True),axis=0).reshape(-1,1)\n",
    "        Y=np.argmax(Y,axis=1).reshape(-1,1)\n",
    "        accuracy=np.sum(Y_==Y)/len(Y_)\n",
    "        return accuracy\n",
    "\n",
    "    \n",
    "\n",
    "    def calcLoss(self,X,Y):\n",
    "        Y_=np.log(self.forward(X.T,predict=True))\n",
    "        loss=np.sum(np.multiply(Y_.T,(-1)*Y))/Y.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 training steps, training accuracy on validate dataset is 15.740000%,loss is 2.471154\n",
      "after 1000 training steps, training accuracy on validate dataset is 97.020000%,loss is 0.092826\n",
      "after 2000 training steps, training accuracy on validate dataset is 97.960000%,loss is 0.071078\n",
      "after 3000 training steps, training accuracy on validate dataset is 98.080000%,loss is 0.069773\n",
      "after 4000 training steps, training accuracy on validate dataset is 97.980000%,loss is 0.068978\n",
      "after 5000 training steps, training accuracy on validate dataset is 98.140000%,loss is 0.068056\n",
      "after 6000 training steps, training accuracy on validate dataset is 98.220000%,loss is 0.070721\n",
      "after 7000 training steps, training accuracy on validate dataset is 98.160000%,loss is 0.069036\n",
      "after 8000 training steps, training accuracy on validate dataset is 98.260000%,loss is 0.068284\n",
      "after 9000 training steps, training accuracy on validate dataset is 98.220000%,loss is 0.070371\n",
      "training complete!Test accuracy is:  98.32\n"
     ]
    }
   ],
   "source": [
    "myANN=annModel(layerDims,Xtrain,ytrain)\n",
    "myANN.GradientDescentOptimizer(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对比一下tensorflow实现的（本机环境为python3.6,tensorflow-gpu1.8）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 training steps,training accuracy on validae dataset is 12.48%\n",
      "after 1000 training steps,training accuracy on validae dataset is 97.32%\n",
      "after 2000 training steps,training accuracy on validae dataset is 97.88%\n",
      "after 3000 training steps,training accuracy on validae dataset is 97.98%\n",
      "after 4000 training steps,training accuracy on validae dataset is 98.22%\n",
      "after 5000 training steps,training accuracy on validae dataset is 98.3%\n",
      "after 6000 training steps,training accuracy on validae dataset is 98.3%\n",
      "after 7000 training steps,training accuracy on validae dataset is 98.38%\n",
      "after 8000 training steps,training accuracy on validae dataset is 98.34%\n",
      "after 9000 training steps,training accuracy on validae dataset is 98.4%\n",
      "training complete!Test accuracy is:  98.08\n"
     ]
    }
   ],
   "source": [
    "INPUT_NODE=784\n",
    "OUTPUT_NODE=10\n",
    "LEARNING_RATE_BASE=0.8\n",
    "TRAINING_STEPS=10000\n",
    "BATCH_SIZE=100\n",
    "\n",
    "\n",
    "def forward(X,layerDims):\n",
    "    AL=X\n",
    "    L=len(layerDims)\n",
    "    for dim in range(1,L):\n",
    "        with tf.variable_scope(('layer'+str(dim)),reuse=tf.AUTO_REUSE):\n",
    "            weights=tf.get_variable(\"weights\",[layerDims[dim-1],layerDims[dim]],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            biases=tf.get_variable(\"biases\",[layerDims[dim]],initializer=tf.constant_initializer(0.0))\n",
    "            AL=tf.matmul(AL,weights)+biases\n",
    "            if dim!=(L-1):\n",
    "                AL=tf.nn.relu(AL)\n",
    "    return AL\n",
    "\n",
    "\n",
    "def train():\n",
    "    X = tf.placeholder(tf.float32, shape=[None,INPUT_NODE], name=\"X_input\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None,OUTPUT_NODE], name=\"y_input\")\n",
    "    h = forward(X, layerDims)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=tf.argmax(y,1))\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    train_step=tf.train.GradientDescentOptimizer(LEARNING_RATE_BASE).minimize(loss)\n",
    "    correct_prediction=tf.equal(tf.argmax(h,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))*100\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        validate_feed = {X: mnist.validation.images, y: mnist.validation.labels}\n",
    "        test_feed = {X: mnist.test.images,y: mnist.test.labels}\n",
    "\n",
    "        for step in range(TRAINING_STEPS):\n",
    "            if step%1000==0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"after %d training steps,training accuracy on validae dataset is %g%%\" % (step, validate_acc))\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_step,feed_dict={X:xs,y:ys})\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"training complete!Test accuracy is: \",test_acc)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 training steps, training accuracy on validate dataset is 18.9%,loss is 2.263199\n",
      "after 1000 training steps, training accuracy on validate dataset is 96.98%,loss is 0.096090\n",
      "after 2000 training steps, training accuracy on validate dataset is 97.7%,loss is 0.076520\n",
      "after 3000 training steps, training accuracy on validate dataset is 97.54%,loss is 0.075833\n",
      "after 4000 training steps, training accuracy on validate dataset is 98.36%,loss is 0.062417\n",
      "after 5000 training steps, training accuracy on validate dataset is 98%,loss is 0.073629\n",
      "after 6000 training steps, training accuracy on validate dataset is 97.98%,loss is 0.081794\n",
      "after 7000 training steps, training accuracy on validate dataset is 98.14%,loss is 0.076164\n",
      "after 8000 training steps, training accuracy on validate dataset is 98.28%,loss is 0.074135\n",
      "after 9000 training steps, training accuracy on validate dataset is 98.08%,loss is 0.085459\n",
      "training complete!Test accuracy is:  98.28\n"
     ]
    }
   ],
   "source": [
    "myANN=annModel(layerDims,Xtrain,ytrain)\n",
    "myANN.AdamOptimizer(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对比一下tensorflow实现的（本机环境为python3.6,tensorflow-gpu1.8）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 training steps,training accuracy on validae dataset is 10.92%\n",
      "after 1000 training steps,training accuracy on validae dataset is 97.32%\n",
      "after 2000 training steps,training accuracy on validae dataset is 97.74%\n",
      "after 3000 training steps,training accuracy on validae dataset is 98.06%\n",
      "after 4000 training steps,training accuracy on validae dataset is 98.36%\n",
      "after 5000 training steps,training accuracy on validae dataset is 98.02%\n",
      "after 6000 training steps,training accuracy on validae dataset is 98.16%\n",
      "after 7000 training steps,training accuracy on validae dataset is 98.38%\n",
      "after 8000 training steps,training accuracy on validae dataset is 98.3%\n",
      "after 9000 training steps,training accuracy on validae dataset is 98.22%\n",
      "training complete!Test accuracy is:  98.16\n"
     ]
    }
   ],
   "source": [
    "INPUT_NODE=784\n",
    "OUTPUT_NODE=10\n",
    "LEARNING_RATE_BASE=0.001\n",
    "TRAINING_STEPS=10000\n",
    "BATCH_SIZE=100\n",
    "\n",
    "\n",
    "def forward(X,layerDims):\n",
    "    AL=X\n",
    "    L=len(layerDims)\n",
    "    for dim in range(1,L):\n",
    "        with tf.variable_scope(('layer'+str(dim)),reuse=tf.AUTO_REUSE):\n",
    "            weights=tf.get_variable(\"weights\",[layerDims[dim-1],layerDims[dim]],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            biases=tf.get_variable(\"biases\",[layerDims[dim]],initializer=tf.constant_initializer(0.0))\n",
    "            AL=tf.matmul(AL,weights)+biases\n",
    "            if dim!=(L-1):\n",
    "                AL=tf.nn.relu(AL)\n",
    "    return AL\n",
    "\n",
    "\n",
    "def train():\n",
    "    X = tf.placeholder(tf.float32, shape=[None,INPUT_NODE], name=\"X_input\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None,OUTPUT_NODE], name=\"y_input\")\n",
    "    h = forward(X, layerDims)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=tf.argmax(y,1))\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    train_step=tf.train.AdamOptimizer(LEARNING_RATE_BASE).minimize(loss)\n",
    "    correct_prediction=tf.equal(tf.argmax(h,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))*100\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        validate_feed = {X: mnist.validation.images, y: mnist.validation.labels}\n",
    "        test_feed = {X: mnist.test.images,y: mnist.test.labels}\n",
    "\n",
    "        for step in range(TRAINING_STEPS):\n",
    "            if step%1000==0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"after %d training steps,training accuracy on validae dataset is %g%%\" % (step, validate_acc))\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_step,feed_dict={X:xs,y:ys})\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"training complete!Test accuracy is: \",test_acc)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 顺便跑下实现的Momentum和RMSprop，篇幅原因就不对比tensorflow了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 training steps, training accuracy on validate dataset is 16.16%,loss is 2.266412\n",
      "after 1000 training steps, training accuracy on validate dataset is 97.46%,loss is 0.090930\n",
      "after 2000 training steps, training accuracy on validate dataset is 97.56%,loss is 0.085476\n",
      "after 3000 training steps, training accuracy on validate dataset is 98.06%,loss is 0.067338\n",
      "after 4000 training steps, training accuracy on validate dataset is 98.42%,loss is 0.065803\n",
      "after 5000 training steps, training accuracy on validate dataset is 98.08%,loss is 0.069263\n",
      "after 6000 training steps, training accuracy on validate dataset is 98.18%,loss is 0.066537\n",
      "after 7000 training steps, training accuracy on validate dataset is 98.44%,loss is 0.063063\n",
      "after 8000 training steps, training accuracy on validate dataset is 98.46%,loss is 0.062138\n",
      "after 9000 training steps, training accuracy on validate dataset is 98.48%,loss is 0.064127\n",
      "training complete!Test accuracy is:  98.35\n"
     ]
    }
   ],
   "source": [
    "myANN=annModel(layerDims,Xtrain,ytrain)\n",
    "myANN.MomentumOptimizer(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 training steps, training accuracy on validate dataset is 24.44%,loss is 7.492584\n",
      "after 1000 training steps, training accuracy on validate dataset is 97.22%,loss is 0.094276\n",
      "after 2000 training steps, training accuracy on validate dataset is 98.04%,loss is 0.070703\n",
      "after 3000 training steps, training accuracy on validate dataset is 98.16%,loss is 0.062427\n",
      "after 4000 training steps, training accuracy on validate dataset is 98.24%,loss is 0.064905\n",
      "after 5000 training steps, training accuracy on validate dataset is 97.96%,loss is 0.067854\n",
      "after 6000 training steps, training accuracy on validate dataset is 98.18%,loss is 0.066930\n",
      "after 7000 training steps, training accuracy on validate dataset is 98.42%,loss is 0.065285\n",
      "after 8000 training steps, training accuracy on validate dataset is 98.1%,loss is 0.067979\n",
      "after 9000 training steps, training accuracy on validate dataset is 98.24%,loss is 0.070708\n",
      "training complete!Test accuracy is:  98.33\n"
     ]
    }
   ],
   "source": [
    "myANN=annModel(layerDims,Xtrain,ytrain)\n",
    "myANN.RMSpropOptimizer(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
